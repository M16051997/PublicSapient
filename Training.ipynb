{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad113333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 0 (0 bytes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No matching packages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip cache purge\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8385a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SIMPLE LOAN DEFAULT PREDICTION - 3 MODELS APPROACH\n",
    "No SMOTE, No Complex Techniques - Just Good Models + Class Weights\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, auc, f1_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84652ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAN DEFAULT PREDICTION - SIMPLE & EFFECTIVE APPROACH\n",
      "================================================================================\n",
      "\n",
      "üìä Step 1: Loading processed data...\n",
      "\n",
      "‚úì Data loaded successfully!\n",
      "  - Features (X): (121856, 54)\n",
      "  - Target (y): (121856,)\n",
      "\n",
      "‚úì Target distribution:\n",
      "  - Non-Default (0): 112,011 (91.9%)\n",
      "  - Default (1): 9,845 (8.1%)\n",
      "  - Imbalance Ratio: 11.4:1\n",
      "\n",
      "================================================================================\n",
      "üìä Step 2: Train/Test Split (Stratified)\n",
      "================================================================================\n",
      "\n",
      "‚úì Split complete!\n",
      "  - Train: 97,484 samples (80%)\n",
      "  - Test:  24,372 samples (20%)\n",
      "\n",
      "‚úì Class distribution maintained:\n",
      "  Train - Default: 7,876 (8.1%)\n",
      "  Test  - Default: 1,969 (8.1%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOAN DEFAULT PREDICTION - SIMPLE & EFFECTIVE APPROACH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD PROCESSED DATA\n",
    "# ============================================================================\n",
    "print(\"\\nüìä Step 1: Loading processed data...\")\n",
    "\n",
    "X = pd.read_csv('processed_data/X_selected.csv') # \\X_selected.csv\n",
    "y = pd.read_csv('processed_data/y_target.csv').values.ravel()\n",
    "\n",
    "print(f\"\\n‚úì Data loaded successfully!\")\n",
    "print(f\"  - Features (X): {X.shape}\")\n",
    "print(f\"  - Target (y): {y.shape}\")\n",
    "print(f\"\\n‚úì Target distribution:\")\n",
    "print(f\"  - Non-Default (0): {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"  - Default (1): {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"  - Imbalance Ratio: {(y == 0).sum()/(y == 1).sum():.1f}:1\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: TRAIN/TEST SPLIT (Stratified)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Step 2: Train/Test Split (Stratified)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 80/20 split, stratified to maintain 11.4:1 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # CRITICAL: Maintains class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Split complete!\")\n",
    "print(f\"  - Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"  - Test:  {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\n‚úì Class distribution maintained:\")\n",
    "print(f\"  Train - Default: {(y_train == 1).sum():,} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Test  - Default: {(y_test == 1).sum():,} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1541b",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "868fe3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: Loading processed data for encoding...\n",
      "================================================================================\n",
      "Data loaded: (121856, 54), Target: (121856,)\n",
      "\n",
      "Fitting encoders only on training data...\n",
      "Encoding complete!\n",
      "Train Encoded Shape: (97484, 56)\n",
      "Test Encoded Shape:  (24372, 56)\n",
      "\n",
      "‚úÖ Encoded datasets and encoder saved successfully!\n",
      "   - artifacts/categorical_encoder.pkl\n",
      "   - processed_data/X_train_encoded.csv\n",
      "   - processed_data/X_test_encoded.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =====================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: Loading processed data for encoding...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "X = pd.read_csv('processed_data/X_selected.csv')\n",
    "y = pd.read_csv('processed_data/y_target.csv').values.ravel()\n",
    "\n",
    "print(f\"Data loaded: {X.shape}, Target: {y.shape}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 2: TRAIN/TEST SPLIT\n",
    "# =====================================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 3: DEFINE CATEGORICAL GROUPS\n",
    "# =====================================================================\n",
    "onehot_features = ['Client_Gender', 'Loan_Contract_Type']\n",
    "ordinal_features = [\n",
    "    'Accompany_Client', 'Client_Income_Type', 'Client_Education',\n",
    "    'Client_Marital_Status', 'Client_Housing_Type',\n",
    "    'Client_Occupation', 'Type_Organization'\n",
    "]\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 4: DEFINE PREPROCESSING PIPELINES\n",
    "# =====================================================================\n",
    "# One-hot encoder pipeline\n",
    "onehot_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Ordinal encoder pipeline\n",
    "ordinal_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# Combine into a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', onehot_pipeline, onehot_features),\n",
    "        ('ordinal', ordinal_pipeline, ordinal_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # keep numerical columns as-is\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 5: FIT ENCODER ONLY ON TRAINING DATA (to prevent leakage)\n",
    "# =====================================================================\n",
    "print(\"\\nFitting encoders only on training data...\")\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert to DataFrame for inspection\n",
    "encoded_feature_names = (\n",
    "    preprocessor.named_transformers_['onehot']\n",
    "    .named_steps['encoder']\n",
    "    .get_feature_names_out(onehot_features)\n",
    ")\n",
    "\n",
    "final_columns = list(encoded_feature_names) + ordinal_features + [\n",
    "    col for col in X.columns if col not in (onehot_features + ordinal_features)\n",
    "]\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=final_columns)\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=final_columns)\n",
    "\n",
    "print(f\"Encoding complete!\")\n",
    "print(f\"Train Encoded Shape: {X_train_encoded.shape}\")\n",
    "print(f\"Test Encoded Shape:  {X_test_encoded.shape}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 6: SAVE ENCODERS & ENCODED DATA\n",
    "# =====================================================================\n",
    "joblib.dump(preprocessor, 'artifacts/categorical_encoder.pkl')\n",
    "X_train_encoded.to_csv('processed_data/X_train_encoded.csv', index=False)\n",
    "X_test_encoded.to_csv('processed_data/X_test_encoded.csv', index=False)\n",
    "pd.DataFrame(y_train).to_csv('processed_data/y_train.csv', index=False)\n",
    "pd.DataFrame(y_test).to_csv('processed_data/y_test.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Encoded datasets and encoder saved successfully!\")\n",
    "print(\"   - artifacts/categorical_encoder.pkl\")\n",
    "print(\"   - processed_data/X_train_encoded.csv\")\n",
    "print(\"   - processed_data/X_test_encoded.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacfb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type_Organization\n",
       "Business Entity Type 3    26279\n",
       "Not_Disclosed             24688\n",
       "Self-employed             14725\n",
       "Other                      6290\n",
       "Medicine                   4320\n",
       "Business Entity Type 2     4126\n",
       "Government                 3971\n",
       "School                     3371\n",
       "Trade: type 7              2979\n",
       "Kindergarten               2686\n",
       "Construction               2623\n",
       "Business Entity Type 1     2313\n",
       "Transport: type 4          2076\n",
       "Trade: type 3              1338\n",
       "Security                   1284\n",
       "Industry: type 9           1280\n",
       "Industry: type 3           1235\n",
       "Housing                    1162\n",
       "Military                   1031\n",
       "Bank                       1012\n",
       "Agriculture                1011\n",
       "Industry: type 11           999\n",
       "Police                      934\n",
       "Postal                      834\n",
       "Transport: type 2           811\n",
       "Security Ministries         756\n",
       "Trade: type 2               717\n",
       "Restaurant                  710\n",
       "Services                    570\n",
       "University                  559\n",
       "Transport: type 3           501\n",
       "Industry: type 7            497\n",
       "Industry: type 1            401\n",
       "Hotel                       393\n",
       "Electricity                 366\n",
       "Industry: type 4            337\n",
       "Trade: type 6               249\n",
       "Industry: type 5            232\n",
       "Telecom                     225\n",
       "Insurance                   215\n",
       "Emergency                   207\n",
       "Industry: type 2            172\n",
       "Realtor                     156\n",
       "Industry: type 12           153\n",
       "Advertising                 152\n",
       "Trade: type 1               135\n",
       "Culture                     131\n",
       "Mobile                      122\n",
       "Legal Services              119\n",
       "Cleaning                    118\n",
       "Transport: type 1            77\n",
       "Industry: type 6             40\n",
       "Industry: type 10            40\n",
       "Religion                     37\n",
       "Industry: type 13            35\n",
       "Trade: type 4                28\n",
       "Trade: type 5                12\n",
       "Industry: type 8             10\n",
       "Not_Applicable                6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X['Accompany_Client'].value_counts() #  6 categories Label Encoding\n",
    "# X['Client_Income_Type'].value_counts() #  8 categories Label Encoding\n",
    "# X['Client_Education'].value_counts() # 4 categories Label Encoding\n",
    "# X['Client_Marital_Status'].value_counts() # 4 categories Label Encoding\n",
    "# X['Client_Gender'].value_counts() # 2 categories onehot Encoding\n",
    "# X['Loan_Contract_Type'].value_counts() # 2 categories onehot Encoding\n",
    "# X['Client_Housing_Type'].value_counts() # 6 categories Label Encoding\n",
    "# X['Client_Occupation'].value_counts() # more than 16 categories Label Encoding\n",
    "# X['Type_Organization'].value_counts() # more than 26 categories Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd542c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üè¶ LOAN DEFAULT PREDICTION - BOOSTING MODEL TRAINING PIPELINE\n",
      "====================================================================================================\n",
      "\n",
      "üì• Loading encoded data...\n",
      "‚úì Training data shape: (97484, 56)\n",
      "‚úì Testing  data shape: (24372, 56)\n",
      "‚úì Target imbalance ratio: 11.4:1\n",
      "\n",
      "‚öñÔ∏è Calculated scale_pos_weight: 11.38\n",
      "\n",
      "====================================================================================================\n",
      "üöÄ Training Model: Logistic Regression\n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Model trained in 14.52s\n",
      "üéØ AUC-ROC: 0.6297 | AUC-PR: 0.1199\n",
      "üìä Recall (Default): 62.01%\n",
      "üìä Precision (Default): 11.59%\n",
      "üìä F1-Score (Default): 0.1953\n",
      "\n",
      "üìã Confusion Matrix:\n",
      "                 Predicted\n",
      "               No Default  Default\n",
      "  Actual No    13,089     9,314\n",
      "         Yes      748     1,221\n",
      "\n",
      "‚ö†Ô∏è Missed Defaults: 748 √ó $315,000 = $235,620,000\n",
      "\n",
      "====================================================================================================\n",
      "üöÄ Training Model: XGBoost\n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Model trained in 4.67s\n",
      "üéØ AUC-ROC: 0.7672 | AUC-PR: 0.2490\n",
      "üìä Recall (Default): 79.69%\n",
      "üìä Precision (Default): 14.46%\n",
      "üìä F1-Score (Default): 0.2448\n",
      "\n",
      "üìã Confusion Matrix:\n",
      "                 Predicted\n",
      "               No Default  Default\n",
      "  Actual No    13,120     9,283\n",
      "         Yes      400     1,569\n",
      "\n",
      "‚ö†Ô∏è Missed Defaults: 400 √ó $315,000 = $126,000,000\n",
      "\n",
      "====================================================================================================\n",
      "üöÄ Training Model: LightGBM\n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Model trained in 2.02s\n",
      "üéØ AUC-ROC: 0.7652 | AUC-PR: 0.2459\n",
      "üìä Recall (Default): 62.16%\n",
      "üìä Precision (Default): 19.41%\n",
      "üìä F1-Score (Default): 0.2959\n",
      "\n",
      "üìã Confusion Matrix:\n",
      "                 Predicted\n",
      "               No Default  Default\n",
      "  Actual No    17,322     5,081\n",
      "         Yes      745     1,224\n",
      "\n",
      "‚ö†Ô∏è Missed Defaults: 745 √ó $315,000 = $234,675,000\n",
      "üíæ Saved Logistic Regression model ‚Üí artifacts/models/logistic_regression.pkl\n",
      "üíæ Saved XGBoost model ‚Üí artifacts/models/xgboost.pkl\n",
      "üíæ Saved LightGBM model ‚Üí artifacts/models/lightgbm.pkl\n",
      "\n",
      "üìä Summary of Model Performance:\n",
      "                 Model   AUC-ROC    AUC-PR  Recall (Default)  \\\n",
      "0  Logistic Regression  0.629730  0.119894          0.620112   \n",
      "1              XGBoost  0.767179  0.249020          0.796851   \n",
      "2             LightGBM  0.765238  0.245860          0.621635   \n",
      "\n",
      "   Precision (Default)  F1-Score (Default)  Train Time (s)  \n",
      "0             0.115899            0.195298           14.52  \n",
      "1             0.144582            0.244755            4.67  \n",
      "2             0.194132            0.295867            2.02  \n",
      "\n",
      "‚úÖ Pipeline completed successfully! All models saved for deployment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, auc,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"üè¶ LOAN DEFAULT PREDICTION - BOOSTING MODEL TRAINING PIPELINE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 1: LOAD ENCODED DATA\n",
    "# =====================================================================\n",
    "print(\"\\nüì• Loading encoded data...\")\n",
    "\n",
    "X_train = pd.read_csv('processed_data/X_train_encoded.csv')\n",
    "X_test = pd.read_csv('processed_data/X_test_encoded.csv')\n",
    "y_train = pd.read_csv('processed_data/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('processed_data/y_test.csv').values.ravel()\n",
    "\n",
    "print(f\"‚úì Training data shape: {X_train.shape}\")\n",
    "print(f\"‚úì Testing  data shape: {X_test.shape}\")\n",
    "print(f\"‚úì Target imbalance ratio: {(y_train==0).sum() / (y_train==1).sum():.1f}:1\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 2: MODEL CONFIGURATION\n",
    "# =====================================================================\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"\\n‚öñÔ∏è Calculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "scale_pos_weight = 20\n",
    "\n",
    "models_config = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        class_weight='balanced', max_iter=1000, random_state=42\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=500,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='auc',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        is_unbalance=True,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 3: TRAINING LOOP\n",
    "# =====================================================================\n",
    "def train_and_evaluate(models, X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "    trained_models = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(f\"üöÄ Training Model: {name}\")\n",
    "        print(\"=\"*100)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        auc_pr = auc(recall_vals, precision_vals)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Log results\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"AUC-ROC\": auc_roc,\n",
    "            \"AUC-PR\": auc_pr,\n",
    "            \"Recall (Default)\": report['1']['recall'],\n",
    "            \"Precision (Default)\": report['1']['precision'],\n",
    "            \"F1-Score (Default)\": report['1']['f1-score'],\n",
    "            \"Train Time (s)\": round(duration, 2)\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n‚úÖ Model trained in {duration:.2f}s\")\n",
    "        print(f\"üéØ AUC-ROC: {auc_roc:.4f} | AUC-PR: {auc_pr:.4f}\")\n",
    "        print(f\"üìä Recall (Default): {report['1']['recall']:.2%}\")\n",
    "        print(f\"üìä Precision (Default): {report['1']['precision']:.2%}\")\n",
    "        print(f\"üìä F1-Score (Default): {report['1']['f1-score']:.4f}\")\n",
    "        print(f\"\\nüìã Confusion Matrix:\")\n",
    "        print(f\"                 Predicted\")\n",
    "        print(f\"               No Default  Default\")\n",
    "        print(f\"  Actual No    {cm[0,0]:>6,}    {cm[0,1]:>6,}\")\n",
    "        print(f\"         Yes   {cm[1,0]:>6,}    {cm[1,1]:>6,}\")\n",
    "        print(f\"\\n‚ö†Ô∏è Missed Defaults: {cm[1,0]:,} √ó $315,000 = ${cm[1,0]*315000:,.0f}\")\n",
    "\n",
    "        # Store trained model\n",
    "        trained_models[name] = model\n",
    "\n",
    "    return pd.DataFrame(results), trained_models\n",
    "\n",
    "\n",
    "results_df, trained_models = train_and_evaluate(models_config, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 4: SAVE MODELS AND RESULTS\n",
    "# =====================================================================\n",
    "os.makedirs(\"artifacts/models\", exist_ok=True)\n",
    "os.makedirs(\"artifacts/reports\", exist_ok=True)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    path = f\"artifacts/models/{name.replace(' ', '_').lower()}.pkl\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"üíæ Saved {name} model ‚Üí {path}\")\n",
    "\n",
    "results_df.to_csv(\"artifacts/reports/model_results.csv\", index=False)\n",
    "print(\"\\nüìä Summary of Model Performance:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline completed successfully! All models saved for deployment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb63beef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üè¶ LOAN DEFAULT PREDICTION - BOOSTING MODEL TUNING PIPELINE\n",
      "====================================================================================================\n",
      "\n",
      "üì• Loading encoded data...\n",
      "‚úì Training data shape: (97484, 56)\n",
      "‚úì Testing  data shape: (24372, 56)\n",
      "‚úì Target imbalance ratio: 11.4:1\n",
      "‚öñÔ∏è scale_pos_weight = 11.38\n",
      "\n",
      "====================================================================================================\n",
      "üöÄ Hyperparameter Tuning: XGBoost with Stratified CV\n",
      "====================================================================================================\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "‚úÖ Best XGBoost AUC-ROC: 0.7727\n",
      "üèÜ Best XGBoost Params: {'subsample': 0.8, 'scale_pos_weight': np.float64(11.377348908075165), 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 8, 'learning_rate': 0.03, 'gamma': 0, 'colsample_bytree': 0.8}\n",
      "‚è±Ô∏è Training Time: 429.76s\n",
      "\n",
      "====================================================================================================\n",
      "üöÄ Hyperparameter Tuning: LightGBM with Stratified CV\n",
      "====================================================================================================\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 150 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n150 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\sklearn.py\", line 1560, in fit\n    super().fit(\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\sklearn.py\", line 1049, in fit\n    self._Booster = train(\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\engine.py\", line 297, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\basic.py\", line 3660, in __init__\n    _safe_call(\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\basic.py\", line 313, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: Cannot set is_unbalance and scale_pos_weight at the same time\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 127\u001b[0m\n\u001b[0;32m    115\u001b[0m lgbm_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m    116\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mlgbm,\n\u001b[0;32m    117\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mlgbm_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m    124\u001b[0m )\n\u001b[0;32m    126\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 127\u001b[0m \u001b[43mlgbm_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m lgbm_duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Best LightGBM AUC-ROC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlgbm_search\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1992\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1992\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1994\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1995\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1996\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1028\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m   1026\u001b[0m     )\n\u001b[1;32m-> 1028\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:505\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    499\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m     )\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 150 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n150 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\sklearn.py\", line 1560, in fit\n    super().fit(\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\sklearn.py\", line 1049, in fit\n    self._Booster = train(\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\engine.py\", line 297, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\basic.py\", line 3660, in __init__\n    _safe_call(\n  File \"c:\\Users\\MoorthyMitturu\\OneDrive - Aionos\\Documents\\PublicSapient\\.venv\\lib\\site-packages\\lightgbm\\basic.py\", line 313, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: Cannot set is_unbalance and scale_pos_weight at the same time\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"üè¶ LOAN DEFAULT PREDICTION - BOOSTING MODEL TUNING PIPELINE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 1: LOAD ENCODED DATA\n",
    "# =====================================================================\n",
    "print(\"\\nüì• Loading encoded data...\")\n",
    "X_train = pd.read_csv('processed_data/X_train_encoded.csv')\n",
    "X_test = pd.read_csv('processed_data/X_test_encoded.csv')\n",
    "y_train = pd.read_csv('processed_data/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('processed_data/y_test.csv').values.ravel()\n",
    "\n",
    "print(f\"‚úì Training data shape: {X_train.shape}\")\n",
    "print(f\"‚úì Testing  data shape: {X_test.shape}\")\n",
    "print(f\"‚úì Target imbalance ratio: {(y_train==0).sum() / (y_train==1).sum():.1f}:1\")\n",
    "\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"‚öñÔ∏è scale_pos_weight = {scale_pos_weight:.2f}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 2: STRATIFIED CROSS VALIDATION SETUP\n",
    "# =====================================================================\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 3: DEFINE HYPERPARAMETER GRIDS\n",
    "# =====================================================================\n",
    "xgb_params = {\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'n_estimators': [300, 500, 700],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'scale_pos_weight': [scale_pos_weight, scale_pos_weight * 1.2, scale_pos_weight * 1.5]\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'max_depth': [4, 6, 8, -1],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'n_estimators': [300, 500, 700],\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'min_child_samples': [20, 50, 100],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 4: RANDOMIZED SEARCH CV - XGBOOST\n",
    "# =====================================================================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üöÄ Hyperparameter Tuning: XGBoost with Stratified CV\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=30,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_search.fit(X_train, y_train)\n",
    "xgb_duration = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Best XGBoost AUC-ROC: {xgb_search.best_score_:.4f}\")\n",
    "print(f\"üèÜ Best XGBoost Params: {xgb_search.best_params_}\")\n",
    "print(f\"‚è±Ô∏è Training Time: {xgb_duration:.2f}s\")\n",
    "\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 5: RANDOMIZED SEARCH CV - LIGHTGBM\n",
    "# =====================================================================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üöÄ Hyperparameter Tuning: LightGBM with Stratified CV\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    is_unbalance=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_search = RandomizedSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_distributions=lgbm_params,\n",
    "    n_iter=30,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "lgbm_search.fit(X_train, y_train)\n",
    "lgbm_duration = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Best LightGBM AUC-ROC: {lgbm_search.best_score_:.4f}\")\n",
    "print(f\"üèÜ Best LightGBM Params: {lgbm_search.best_params_}\")\n",
    "print(f\"‚è±Ô∏è Training Time: {lgbm_duration:.2f}s\")\n",
    "\n",
    "best_lgbm = lgbm_search.best_estimator_\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 6: FINAL EVALUATION ON TEST DATA\n",
    "# =====================================================================\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"üìä Final Evaluation: {name}\")\n",
    "    print(\"=\"*100)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"üéØ AUC-ROC: {auc_roc:.4f}\")\n",
    "    print(f\"üéØ AUC-PR:  {auc_pr:.4f}\")\n",
    "    print(f\"üìà Recall: {report['1']['recall']:.2%}\")\n",
    "    print(f\"üìà Precision: {report['1']['precision']:.2%}\")\n",
    "    print(f\"üìà F1-Score: {report['1']['f1-score']:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'AUC-ROC': auc_roc,\n",
    "        'AUC-PR': auc_pr,\n",
    "        'Recall': report['1']['recall'],\n",
    "        'Precision': report['1']['precision'],\n",
    "        'F1': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "xgb_results = evaluate_model(\"XGBoost (Tuned)\", best_xgb, X_test, y_test)\n",
    "lgbm_results = evaluate_model(\"LightGBM (Tuned)\", best_lgbm, X_test, y_test)\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 7: SAVE BEST MODELS\n",
    "# =====================================================================\n",
    "os.makedirs(\"artifacts/tuned_models\", exist_ok=True)\n",
    "joblib.dump(best_xgb, \"artifacts/tuned_models/xgboost_tuned.pkl\")\n",
    "joblib.dump(best_lgbm, \"artifacts/tuned_models/lightgbm_tuned.pkl\")\n",
    "\n",
    "results_df = pd.DataFrame([xgb_results, lgbm_results])\n",
    "results_df.to_csv(\"artifacts/tuned_models/tuned_results.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ All tuned models trained, evaluated, and saved successfully!\")\n",
    "print(\"\\nüìä Final Summary:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4a351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e557d68",
   "metadata": {},
   "source": [
    "### With Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "630a62c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üè¶ LOAN DEFAULT PREDICTION - BALANCED BOOSTING PIPELINE (SMOTETomek)\n",
      "====================================================================================================\n",
      "Training data: (97484, 56), Imbalance: 11.4:1\n",
      "\n",
      "‚öñÔ∏è Balancing the training data using SMOTETomek...\n",
      "Balanced data shape: (116490, 56)\n",
      "New ratio: 3.33:1\n",
      "\n",
      "====================================================================================================\n",
      "üöÄ Training XGBoost (Balanced)\n",
      "====================================================================================================\n",
      "üéØ AUC-ROC: 0.7695 | AUC-PR: 0.2613\n",
      "üìà Recall: 3.00% | Precision: 57.84% | F1: 0.057\n",
      "‚è±Ô∏è Training Time: 6.86s\n",
      "\n",
      "====================================================================================================\n",
      "üöÄ Training LightGBM (Balanced)\n",
      "====================================================================================================\n",
      "üéØ AUC-ROC: 0.7648 | AUC-PR: 0.2477\n",
      "üìà Recall: 2.95% | Precision: 58.00% | F1: 0.056\n",
      "‚è±Ô∏è Training Time: 4.68s\n",
      "\n",
      "üìä Final Summary:\n",
      "                 Model   AUC-ROC    AUC-PR    Recall  Precision        F1  \\\n",
      "0   XGBoost (Balanced)  0.769467  0.261314  0.029964   0.578431  0.056977   \n",
      "1  LightGBM (Balanced)  0.764772  0.247690  0.029457   0.580000  0.056066   \n",
      "\n",
      "   Train Time (s)  \n",
      "0            6.86  \n",
      "1            4.68  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings, time\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"üè¶ LOAN DEFAULT PREDICTION - BALANCED BOOSTING PIPELINE (SMOTETomek)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ===============================================================\n",
    "# STEP 1: Load Data\n",
    "# ===============================================================\n",
    "X_train = pd.read_csv('processed_data/X_train_encoded.csv')\n",
    "X_test = pd.read_csv('processed_data/X_test_encoded.csv')\n",
    "y_train = pd.read_csv('processed_data/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('processed_data/y_test.csv').values.ravel()\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, Imbalance: {(y_train==0).sum()/(y_train==1).sum():.1f}:1\")\n",
    "\n",
    "# ===============================================================\n",
    "# STEP 2: SMOTETomek Resampling on Training Data\n",
    "# ===============================================================\n",
    "print(\"\\n‚öñÔ∏è Balancing the training data using SMOTETomek...\")\n",
    "# smt = SMOTETomek(random_state=42, sampling_strategy=0.5)  # target 1:2 ratio\n",
    "# X_train_bal, y_train_bal = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.3)  # Don't fully balance\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "\n",
    "print(f\"Balanced data shape: {X_train_resampled.shape}\")\n",
    "print(f\"New ratio: {(y_train_resampled==0).sum()/(y_train_resampled==1).sum():.2f}:1\")\n",
    "\n",
    "# ===============================================================\n",
    "# STEP 3: Train Models\n",
    "# ===============================================================\n",
    "models = {\n",
    "    \"XGBoost (Balanced)\": XGBClassifier(\n",
    "        scale_pos_weight=1.0,  # already balanced\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=600,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric='auc',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"LightGBM (Balanced)\": LGBMClassifier(\n",
    "        is_unbalance=False,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=600,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"üöÄ Training {name}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    start = time.time()\n",
    "    model.fit(X_train_resampled, y_train_resampled) # xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "    dur = time.time() - start\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, y_proba)\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    auc_pr = auc(rec, prec)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"Recall\": report['1']['recall'],\n",
    "        \"Precision\": report['1']['precision'],\n",
    "        \"F1\": report['1']['f1-score'],\n",
    "        \"Train Time (s)\": round(dur, 2)\n",
    "    })\n",
    "    \n",
    "    print(f\"üéØ AUC-ROC: {auc_roc:.4f} | AUC-PR: {auc_pr:.4f}\")\n",
    "    print(f\"üìà Recall: {report['1']['recall']:.2%} | Precision: {report['1']['precision']:.2%} | F1: {report['1']['f1-score']:.3f}\")\n",
    "    print(f\"‚è±Ô∏è Training Time: {dur:.2f}s\")\n",
    "\n",
    "# ===============================================================\n",
    "# STEP 4: Save Results\n",
    "# ===============================================================\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä Final Summary:\")\n",
    "print(results_df)\n",
    "joblib.dump(models, \"artifacts/tuned_models/balanced_boosting.pkl\")\n",
    "results_df.to_csv(\"artifacts/tuned_models/balanced_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab387fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üè¶ LOAN DEFAULT PREDICTION - IMPROVED SMOTE PIPELINE\n",
      "================================================================================\n",
      "Training data: (97484, 56), Imbalance: 11.4:1\n",
      "\n",
      "‚öñÔ∏è Applying SMOTE with conservative ratio...\n",
      "‚úì Balanced data shape: (116490, 56)\n",
      "‚úì New ratio: 3.33:1\n",
      "‚úì Class distribution: [89608 26882]\n",
      "\n",
      "================================================================================\n",
      "üöÄ Training XGBoost (SMOTE + Tuned)\n",
      "================================================================================\n",
      "\n",
      "üéØ Finding Optimal Classification Threshold...\n",
      "\n",
      "üìä Threshold Analysis:\n",
      "  Recall=70%: threshold=0.193, precision=17.3%\n",
      "  Best F1:    threshold=0.327, recall=47.3%, precision=25.6%\n",
      "  Prec>=30%:  threshold=0.412, recall=31.7%\n",
      "\n",
      "Recall_70 (threshold=0.193):\n",
      "  Recall: 70.0% | Precision: 17.3% | F1: 0.278\n",
      "  Confusion Matrix:\n",
      "[[15823  6580]\n",
      " [  591  1378]]\n",
      "\n",
      "Best_F1 (threshold=0.327):\n",
      "  Recall: 47.3% | Precision: 25.6% | F1: 0.333\n",
      "  Confusion Matrix:\n",
      "[[19703  2700]\n",
      " [ 1038   931]]\n",
      "\n",
      "Prec_30 (threshold=0.412):\n",
      "  Recall: 31.7% | Precision: 30.0% | F1: 0.308\n",
      "  Confusion Matrix:\n",
      "[[20945  1458]\n",
      " [ 1344   625]]\n",
      "\n",
      "Default_0.5 (threshold=0.500):\n",
      "  Recall: 20.2% | Precision: 36.2% | F1: 0.259\n",
      "  Confusion Matrix:\n",
      "[[21701   702]\n",
      " [ 1571   398]]\n",
      "\n",
      "üéØ Threshold-Independent Metrics:\n",
      "  AUC-ROC: 0.7731\n",
      "  AUC-PR:  0.2644\n",
      "\n",
      "‚úÖ Model and thresholds saved!\n",
      "  - artifacts/xgboost_smote.pkl\n",
      "  - artifacts/optimal_thresholds.csv\n",
      "  - artifacts/threshold_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "üí° RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üéØ For Maximum Recall (catch more defaulters):\n",
      "   Use threshold: 0.193\n",
      "   Recall: 70.0%\n",
      "   Precision: 17.3%\n",
      "\n",
      "‚öñÔ∏è For Balanced Performance (F1):\n",
      "   Use threshold: 0.327\n",
      "   Recall: 47.3%\n",
      "   Precision: 25.6%\n",
      "\n",
      "üìä Results Summary:\n",
      "Threshold_Strategy  Threshold   Recall  Precision       F1\n",
      "         Recall_70   0.192501 0.699848   0.173159 0.277627\n",
      "           Best_F1   0.327399 0.472829   0.256403 0.332500\n",
      "           Prec_30   0.412306 0.317420   0.300048 0.308490\n",
      "       Default_0.5   0.500000 0.202133   0.361818 0.259368\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, \n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_curve\n",
    ")\n",
    "import time\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =====================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"üè¶ LOAN DEFAULT PREDICTION - IMPROVED SMOTE PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = pd.read_csv('processed_data/X_train_encoded.csv')\n",
    "X_test = pd.read_csv('processed_data/X_test_encoded.csv')\n",
    "y_train = pd.read_csv('processed_data/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('processed_data/y_test.csv').values.ravel()\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, Imbalance: {(y_train==0).sum()/(y_train==1).sum():.1f}:1\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 2: APPLY SMOTE WITH CONSERVATIVE RATIO\n",
    "# =====================================================================\n",
    "print(\"\\n‚öñÔ∏è Applying SMOTE with conservative ratio...\")\n",
    "\n",
    "# Option A: Use moderate sampling (recommended)\n",
    "smote = SMOTE(\n",
    "    sampling_strategy=0.3,  # Bring minority to 30% of majority (not 100%)\n",
    "    random_state=42,\n",
    "    k_neighbors=5\n",
    ")\n",
    "\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"‚úì Balanced data shape: {X_train_balanced.shape}\")\n",
    "print(f\"‚úì New ratio: {(y_train_balanced==0).sum()/(y_train_balanced==1).sum():.2f}:1\")\n",
    "print(f\"‚úì Class distribution: {np.bincount(y_train_balanced)}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 3: TRAIN XGBOOST WITH ADJUSTED PARAMS\n",
    "# =====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Training XGBoost (SMOTE + Tuned)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Adjust scale_pos_weight based on new ratio\n",
    "new_ratio = (y_train_balanced==0).sum() / (y_train_balanced==1).sum()\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    # Use your best params from original tuning\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=5,\n",
    "    gamma=0,\n",
    "    scale_pos_weight=new_ratio,  # Adjust to new ratio\n",
    "    random_state=42,\n",
    "    eval_metric='aucpr',  # Focus on AUC-PR\n",
    "    early_stopping_rounds=50,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "xgb_model.fit(\n",
    "    X_train_balanced, \n",
    "    y_train_balanced,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "train_time = time.time() - start\n",
    "\n",
    "# Get probability predictions\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 4: FIND OPTIMAL THRESHOLD\n",
    "# =====================================================================\n",
    "print(\"\\nüéØ Finding Optimal Classification Threshold...\")\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Strategy 1: Target Recall = 70%\n",
    "target_recall = 0.70\n",
    "idx_recall = np.argmin(np.abs(recalls - target_recall))\n",
    "threshold_recall_70 = thresholds[idx_recall]\n",
    "\n",
    "# Strategy 2: Maximize F1-Score\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "idx_f1 = np.argmax(f1_scores)\n",
    "threshold_f1 = thresholds[idx_f1]\n",
    "\n",
    "# Strategy 3: Target Precision >= 30%\n",
    "valid_idx = np.where(precisions >= 0.30)[0]\n",
    "if len(valid_idx) > 0:\n",
    "    idx_prec = valid_idx[np.argmax(recalls[valid_idx])]\n",
    "    threshold_prec_30 = thresholds[idx_prec]\n",
    "else:\n",
    "    threshold_prec_30 = 0.5\n",
    "\n",
    "print(f\"\\nüìä Threshold Analysis:\")\n",
    "print(f\"  Recall=70%: threshold={threshold_recall_70:.3f}, precision={precisions[idx_recall]:.1%}\")\n",
    "print(f\"  Best F1:    threshold={threshold_f1:.3f}, recall={recalls[idx_f1]:.1%}, precision={precisions[idx_f1]:.1%}\")\n",
    "print(f\"  Prec>=30%:  threshold={threshold_prec_30:.3f}, recall={recalls[idx_prec]:.1%}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 5: EVALUATE WITH MULTIPLE THRESHOLDS\n",
    "# =====================================================================\n",
    "thresholds_to_test = {\n",
    "    'Recall_70': threshold_recall_70,\n",
    "    'Best_F1': threshold_f1,\n",
    "    'Prec_30': threshold_prec_30,\n",
    "    'Default_0.5': 0.5\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, threshold in thresholds_to_test.items():\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "    \n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Threshold_Strategy': name,\n",
    "        'Threshold': threshold,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name} (threshold={threshold:.3f}):\")\n",
    "    print(f\"  Recall: {recall:.1%} | Precision: {precision:.1%} | F1: {f1:.3f}\")\n",
    "    print(f\"  Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Overall metrics (threshold-independent)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "auc_pr = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"\\nüéØ Threshold-Independent Metrics:\")\n",
    "print(f\"  AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"  AUC-PR:  {auc_pr:.4f}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 6: SAVE BEST MODEL WITH RECOMMENDED THRESHOLD\n",
    "# =====================================================================\n",
    "import joblib\n",
    "\n",
    "# Save model\n",
    "joblib.dump(xgb_model, 'artifacts/xgboost_smote.pkl')\n",
    "\n",
    "# Save optimal thresholds\n",
    "threshold_info = pd.DataFrame([{\n",
    "    'strategy': name,\n",
    "    'threshold': threshold,\n",
    "    'description': f'Optimized for {name}'\n",
    "} for name, threshold in thresholds_to_test.items()])\n",
    "\n",
    "threshold_info.to_csv('artifacts/optimal_thresholds.csv', index=False)\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv('artifacts/threshold_comparison.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Model and thresholds saved!\")\n",
    "print(f\"  - artifacts/xgboost_smote.pkl\")\n",
    "print(f\"  - artifacts/optimal_thresholds.csv\")\n",
    "print(f\"  - artifacts/threshold_comparison.csv\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 7: FINAL RECOMMENDATION\n",
    "# =====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best threshold based on business objective\n",
    "best_recall_idx = results_df['Recall'].idxmax()\n",
    "best_f1_idx = results_df['F1'].idxmax()\n",
    "\n",
    "print(f\"\\nüéØ For Maximum Recall (catch more defaulters):\")\n",
    "print(f\"   Use threshold: {results_df.loc[best_recall_idx, 'Threshold']:.3f}\")\n",
    "print(f\"   Recall: {results_df.loc[best_recall_idx, 'Recall']:.1%}\")\n",
    "print(f\"   Precision: {results_df.loc[best_recall_idx, 'Precision']:.1%}\")\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è For Balanced Performance (F1):\")\n",
    "print(f\"   Use threshold: {results_df.loc[best_f1_idx, 'Threshold']:.3f}\")\n",
    "print(f\"   Recall: {results_df.loc[best_f1_idx, 'Recall']:.1%}\")\n",
    "print(f\"   Precision: {results_df.loc[best_f1_idx, 'Precision']:.1%}\")\n",
    "\n",
    "print(\"\\nüìä Results Summary:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PublicSapient (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
